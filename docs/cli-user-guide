
# GuideLLM CLI User Guide 

For more details on setup and installation, see the Setup and [Installation](https://apps.neuralmagic.com/GuideLLM/README.MD/#Installation) sections. 

## About GuideLLM

The GuideLLM CLI is a performance benchmarking tool to enable you to evaluate and visualize your LLM inference serving performance before your deploy to production. GuideLLM runs via the CLI and takes in a wide array of input arguments to enable you to configure your workload to run benchmarks to the specifications that you desire. This ultimately provides the ability to understand bottlenecks in your inference serving pipeline and make changes before your users are effected by your LLM application. 

## GuideLLM CLI Quickstart 

To get started with GuideLLM, you will need to have an inference server installed and available within the same cluster as GuideLLM. GuideLLM will take a target to an inference server as a mandatory argument, so it is important to have both libraries installed on the same GPU/TPU cluster. 

To get started with GuideLLM: 
1. Deploy your model on the inference server of your choice. We recommend [vLLM](https://github.com/vllm-project/vllm). The inference server you choose should be `open-ai server` compatible for the best results. 
2. Pip install guidellm with: `pip install guidellm`. 
3. Once installed, cd into the guidellm directory with: `cd guidellm`. 
4. In order to avoid compatibility issues with guidellm, it is recommended to run it in a fresh [virtual environment](https://docs.python.org/3/library/venv.html).
5. Create a virtual environment with: `virtualenv -p python3.8 venv`. 
6. Now activate the venv with: source `venv/bin/activate`.
7. To run a benchmark on Neural Magic's W4A16 quantized Llama 3.1 8B , you can run the following command: ``python src/guidellm/main.py --target "http://HOST_URL/v1" --model "neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16" --data-type emulated --data "prompt_tokens=512,generated_tokens=128" --max-seconds 60``
	- This benchmark will run with the default values, unless otherwise specified. 
	

## GuideLLM CLI Details
### Input Metrics
The input arguments are split up into 3 sections: 

- **Workload Overview**
- **Workload Data**
- **Workload Type**

Once you fill out these arguments and run the command, GuideLLM will run the simulated workload. Note the time it takes to run can be set with <em>max_seconds</em>, but may also depend on the hardware and model. 

### Workload Overview

This section of input parameters covers what to actually benchmark including the target host location, model, and task. The full list of arguments and their defintions are presented below:

-   **--target** <em>(str, default: localhost with chat completions api for VLLM)</em>: Target for benchmarking 
    
	-   optional breakdown args if target isn't specified:
    
		-   **--host** <em>(str)</em>: Host URL for benchmarking
    
		-   **--port** <em>(str)</em>: Port available for benchmarking
    
-   **--backend** <em>(str, default: server_openai [vllm, TGI, llama.cpp, DeepSparse, and many popular servers match this format])</em>: Backend type for benchmarking
    
-   **--model** <em>(str, default: auto populated from vllm server)</em>: Model being used for benchmarking, running on the inference server
        
-   **--task** <em>(str), optional)</em>: Task to use for benchmarking

-  **--output-path** <em>(str), optional)</em>: Path to save report report to



### Workload Data

This section of input parameters covers the data arguments that need to be supplied such as a reference to the dataset and tokenizer. The list of arguments and their defintions are presented below:

-   **--data** <em>(str)</em>: Data file or alias for benchmarking

-   **--data-type** <em>(ENUM, default: emulated; [file, transformers])</em>: The type of data given for benchmarking

-   **--tokenizer** <em>(str)</em>: Tokenizer to use for benchmarking

### Workload Type

This section of input parameters covers the type of workload that you want to run to represent the type of load you expect on your server in production such as rate-per-second and the frequency of requests. The full list of arguments and their definitions are presented below:

-   **--rate-type**  <em>(ENUM, default: sweep; [serial, constant, poisson] where sweep will cover a range of constant request rates and ensure saturation of server, serial will send one request at a time, constant will send a constant request rate, poisson will send a request rate sampled from a poisson distribution at a given mean) </em>: Type of rate generation for benchmarking
    
-   **--rate** <em>(float)</em>: Rate to use for constant and poisson rate types
    
-   **--max-seconds** <em>(integer)</em>: Number of seconds to result each request rate at
    
-   **--max-requests** <em>(integer)</em>: Number of requests to send for each rate
    
### Output Metrics

Once your GuideLLM run is complete, the output metrics are displayed via the Terminal in the following 4 sections: 

- **Requests Data by Benchmark**
- **Tokens Data by Benchmark**
- **Performance Stats by Benchmark**
- **Performance Summary by Benchmark**

### Requests Data by Benchmark 

This section shows the request statistics for the benchmark that was run. It includes:
- Benchmark: Synchronous or Asynchronous@X req/sec
- Requests Completed 
- Requests Failed
- Duration (sec)
- Start Time (HH:MI:SS)
- End Time (HH:MI:SS)


### Tokens Data by Benchmark
This section shows the prompt and generation token distribution statistics for the benchmark that was run. It includes:
- Benchmark: Synchronous or Asynchronous@X req/sec
- Prompt (token length)
- Prompt (1%, 5%, 50%, 95%, 99%): Distribution of prompt token length
- Output (token length)
- Output (1%, 5%, 50%, 95%, 99%): Distribution of output token length

### Performance Stats by Benchmark
This section shows the prompt and generation token distribution statistics for the benchmark that was run. It includes:
- Benchmark: Synchronous or Asynchronous@X req/sec
- Request Latency [1%, 5%, 10%, 50%, 90%, 95%, 99%] (sec)
-  Time to First Token [1%, 5%, 10%, 50%, 90%, 95%, 99%] (ms)
- Inter Token Latency [1%, 5%, 10%, 50%, 90% 95%, 99%] (ms)
-  Output Token Throughput (tokens/sec)


### Performance Summary by Benchmark
This section shows the prompt and generation token distribution statistics for the benchmark that was run. It includes:
- Benchmark: Synchronous or Asynchronous@X req/sec
- Requests per Second (req/sec)
-  Request Latency (ms)
-  Time to First Token (ms)
-  Inter Token Latency (ms)
-  Output Token Throughput (tokens/sec)



## Report a Bug

To report a bug, file an issue on [GitHub Issues](https://github.com/neuralmagic/guidellm/issues). 



